Nominal Direction Probability: 0.8
Policy Evaluation Iterations perImprovement Step: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 60, 60, 53, 13, 14, 13, 46, 48, 44, 44, 42, 41, 39, 38, 37, 36, 35, 33, 32, 31, 30, 28, 27, 25, 24, 23, 21, 20, 19, 18, 16, 15, 14, 12, 11, 9, 7]
Total Number of Policy Improvement Iterations: 70

The policy avoids walls, preferring to have the robot walk at least one tile away from a barrier like the baggage reclaim (which are heavily penalized) or regular obstacles like trash cans and walls (which are mildly penalized). This can possibly be attributed to the uncertainty: if the robot is walking alongside an obstacle, there is a non-zero probability that the robot can mis-step and be penalized for bumping the obstacle. Passing through customs is a high-cost action, so the policy guides the robot away from the customs area and toward the secret door on the right side to enter the upper area from the lower.